# 实现方案文档（简化版）

## 1. 概述

本文档基于简化版接口设计，说明 Kafka-Flink-Hologres 自动化工具的实现方案。遵循"最小化实现"原则，聚焦核心功能。

## 2. 项目目录结构

```
kafka_flink_hologres/
├── README.md                          # 项目说明文档
├── pyproject.toml                     # 项目配置和依赖
├── config.yaml                        # 配置文件
├── .gitignore                         # Git 忽略文件
│
├── discuss/                           # 讨论和方案文档
│   ├── 01-需求文档.md
│   ├── 02-数据库设计.md
│   ├── 03-接口设计-简化版.md
│   └── 04-实现方案-简化版.md
│
├── scripts/                           # 脚本目录
│   ├── init_database.sql              # 数据库初始化 SQL
│   └── run.sh                         # 运行脚本
│
├── logs/                              # 日志目录
│   └── .gitkeep
│
├── src/                               # 源代码
│   └── kafka_flink_tool/              # 主包
│       ├── __init__.py
│       ├── __main__.py                # CLI 入口
│       │
│       ├── config.py                  # 配置管理（80 行）
│       ├── models.py                  # 数据模型（120 行）
│       ├── database.py                # 数据库访问（200 行）
│       ├── kafka_client.py            # Kafka 客户端（120 行）
│       ├── type_inference.py          # 类型推断（150 行）
│       ├── ddl_generator.py           # DDL 生成（100 行）
│       ├── sql_generator.py           # Flink SQL 生成（200 行）
│       ├── service.py                 # 业务服务（200 行）
│       ├── cli.py                     # CLI 实现（80 行）
│       └── logger.py                  # 日志配置（50 行）
│
└── tests/                             # 测试代码
    ├── __init__.py
    ├── test_type_inference.py
    ├── test_ddl_generator.py
    ├── test_sql_generator.py
    └── test_service.py
```

**总代码行数估算**：约 1300 行（不含测试）

## 3. 核心模块设计

### 3.1 配置管理 (config.py)

**职责**：加载和管理配置

**核心代码**：
```python
import yaml
from pydantic import BaseModel

class HologresConfig(BaseModel):
    host: str
    port: int = 80
    database: str
    user: str
    password: str

class ConfigManager:
    def __init__(self, config_path: str = "config.yaml"):
        self.config_path = config_path
        self._config = None

    def get_hologres_config(self) -> HologresConfig:
        if not self._config:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self._config = yaml.safe_load(f)
        return HologresConfig(**self._config['hologres'])
```

**代码行数**：~80 行

---

### 3.2 数据模型 (models.py)

**职责**：定义所有数据模型

**核心代码**：
```python
from typing import Optional, List, Any, Dict
from pydantic import BaseModel

class KafkaTopicConfig(BaseModel):
    id: int
    topic_name: str
    kafka_brokers: str
    data_format: str
    description: Optional[str] = None
    is_active: bool

class FieldSchema(BaseModel):
    name: str
    type: str  # BIGINT, DOUBLE, TEXT, BOOLEAN, TIMESTAMPTZ
    nullable: bool = True

class InferredSchema(BaseModel):
    fields: List[FieldSchema]
    sample_data_count: int

class FlinkSQLRecord(BaseModel):
    id: Optional[int] = None
    topic_id: int
    topic_name: str
    sink_table_name: str
    source_ddl: str
    sink_ddl: str
    insert_sql: str
    full_sql: str
    inferred_schema: Optional[dict] = None
    sample_count: int = 10
    status: str = "generated"
```

**代码行数**：~120 行

---

### 3.3 数据库访问 (database.py)

**职责**：封装所有数据库操作

**核心代码**：
```python
import psycopg2
from typing import Optional
from .config import HologresConfig
from .models import KafkaTopicConfig, FlinkSQLRecord

class HologresDAO:
    def __init__(self, config: HologresConfig):
        self.config = config
        self._conn = None

    def _get_connection(self):
        if not self._conn or self._conn.closed:
            self._conn = psycopg2.connect(
                host=self.config.host,
                port=self.config.port,
                database=self.config.database,
                user=self.config.user,
                password=self.config.password
            )
        return self._conn

    def get_topic_config_by_name(self, topic_name: str) -> Optional[KafkaTopicConfig]:
        conn = self._get_connection()
        with conn.cursor() as cur:
            cur.execute(
                "SELECT id, topic_name, kafka_brokers, data_format, description, is_active "
                "FROM kafka_topic_config WHERE topic_name = %s AND is_active = true",
                (topic_name,)
            )
            row = cur.fetchone()
            if row:
                return KafkaTopicConfig(
                    id=row[0],
                    topic_name=row[1],
                    kafka_brokers=row[2],
                    data_format=row[3],
                    description=row[4],
                    is_active=row[5]
                )
        return None

    def table_exists(self, table_name: str) -> bool:
        conn = self._get_connection()
        with conn.cursor() as cur:
            cur.execute(
                "SELECT EXISTS (SELECT 1 FROM information_schema.tables "
                "WHERE table_name = %s)",
                (table_name,)
            )
            return cur.fetchone()[0]

    def create_table(self, ddl: str) -> None:
        conn = self._get_connection()
        with conn.cursor() as cur:
            cur.execute(ddl)
        conn.commit()

    def save_flink_sql_record(self, record: FlinkSQLRecord) -> int:
        conn = self._get_connection()
        with conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO flink_sql_record (
                    topic_id, topic_name, sink_table_name,
                    source_ddl, sink_ddl, insert_sql, full_sql,
                    inferred_schema, sample_count, status
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id
                """,
                (
                    record.topic_id, record.topic_name, record.sink_table_name,
                    record.source_ddl, record.sink_ddl, record.insert_sql, record.full_sql,
                    record.inferred_schema, record.sample_count, record.status
                )
            )
            record_id = cur.fetchone()[0]
        conn.commit()
        return record_id

    def close(self):
        if self._conn and not self._conn.closed:
            self._conn.close()
```

**代码行数**：~200 行

---

### 3.4 Kafka 客户端 (kafka_client.py)

**职责**：连接 Kafka 并采样数据

**核心代码**：
```python
import json
from typing import List, Dict, Any
from kafka import KafkaConsumer

class KafkaClient:
    def __init__(self, brokers: str, topic_name: str):
        self.brokers = brokers.split(',')
        self.topic_name = topic_name
        self._consumer = None

    def sample_messages(self, count: int = 10) -> List[Dict[str, Any]]:
        consumer = KafkaConsumer(
            self.topic_name,
            bootstrap_servers=self.brokers,
            auto_offset_reset='latest',
            enable_auto_commit=False,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )

        messages = []
        try:
            for message in consumer:
                messages.append(message.value)
                if len(messages) >= count:
                    break
        finally:
            consumer.close()

        if len(messages) < count:
            raise ValueError(f"采样数据不足，期望 {count} 条，实际 {len(messages)} 条")

        return messages
```

**代码行数**：~120 行

---

### 3.5 类型推断 (type_inference.py)

**职责**：推断数据类型

**核心代码**：
```python
from typing import List, Dict, Any
from collections import defaultdict
from datetime import datetime
from .models import FieldSchema, InferredSchema

class TypeInferencer:
    def infer_schema(self, messages: List[Dict[str, Any]]) -> InferredSchema:
        # 1. 收集所有字段的值
        field_values = defaultdict(list)
        for msg in messages:
            for key, value in msg.items():
                field_values[key].append(value)

        # 2. 推断每个字段的类型
        fields = []
        for field_name, values in field_values.items():
            field_type = self._infer_field_type(values)
            fields.append(FieldSchema(
                name=field_name,
                type=field_type,
                nullable=True
            ))

        return InferredSchema(
            fields=fields,
            sample_data_count=len(messages)
        )

    def _infer_field_type(self, values: List[Any]) -> str:
        # 收集非 None 值的类型
        types = set()
        for v in values:
            if v is not None:
                types.add(type(v).__name__)

        # 应用宽松类型策略
        if 'float' in types:
            return 'DOUBLE'
        elif 'int' in types and 'float' not in types:
            return 'BIGINT'
        elif 'bool' in types and len(types) == 1:
            return 'BOOLEAN'
        elif 'str' in types:
            # 检查是否是时间戳格式
            for v in values:
                if isinstance(v, str) and self._is_timestamp(v):
                    return 'TIMESTAMPTZ'
            return 'TEXT'
        else:
            return 'TEXT'

    def _is_timestamp(self, value: str) -> bool:
        timestamp_formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%d',
        ]
        for fmt in timestamp_formats:
            try:
                datetime.strptime(value, fmt)
                return True
            except:
                continue
        return False
```

**代码行数**：~150 行

---

### 3.6 DDL 生成 (ddl_generator.py)

**职责**：生成 Hologres DDL

**核心代码**：
```python
from .models import InferredSchema

class DDLGenerator:
    def generate_hologres_ddl(self, table_name: str, schema: InferredSchema) -> str:
        lines = [f"CREATE TABLE IF NOT EXISTS {table_name} ("]

        field_defs = []
        for field in schema.fields:
            nullable = "" if field.nullable else "NOT NULL"
            field_defs.append(f"    {field.name} {field.type} {nullable}".strip())

        lines.append(",\n".join(field_defs))
        lines.append(");")

        ddl = "\n".join(lines)

        # 添加注释
        comments = [f"\nCOMMENT ON TABLE {table_name} IS '从 Kafka 同步的数据表';"]
        for field in schema.fields:
            comments.append(
                f"COMMENT ON COLUMN {table_name}.{field.name} IS '{field.name} 字段';"
            )

        return ddl + "\n" + "\n".join(comments)
```

**代码行数**：~100 行

---

### 3.7 Flink SQL 生成 (sql_generator.py)

**职责**：生成 Flink SQL

**核心代码**：
```python
from .models import InferredSchema
from .config import HologresConfig

class FlinkSQLGenerator:
    # Hologres 类型到 Flink 类型的映射
    TYPE_MAPPING = {
        'BIGINT': 'BIGINT',
        'DOUBLE': 'DOUBLE',
        'TEXT': 'STRING',
        'BOOLEAN': 'BOOLEAN',
        'TIMESTAMPTZ': 'TIMESTAMP(3)'
    }

    def generate_full_sql(
        self,
        topic_name: str,
        sink_table: str,
        schema: InferredSchema,
        kafka_brokers: str,
        hologres_config: HologresConfig
    ) -> tuple[str, str, str, str]:

        source_ddl = self._generate_source_ddl(topic_name, schema, kafka_brokers)
        sink_ddl = self._generate_sink_ddl(sink_table, schema, hologres_config)
        insert_sql = self._generate_insert_sql(topic_name, sink_table, schema)
        full_sql = f"{source_ddl}\n\n{sink_ddl}\n\n{insert_sql}"

        return source_ddl, sink_ddl, insert_sql, full_sql

    def _generate_source_ddl(self, topic_name: str, schema: InferredSchema, brokers: str) -> str:
        source_table = f"kafka_source_{topic_name}"

        fields = []
        for field in schema.fields:
            flink_type = self.TYPE_MAPPING.get(field.type, 'STRING')
            fields.append(f"    {field.name} {flink_type}")

        fields_str = ",\n".join(fields)

        return f"""CREATE TABLE {source_table} (
{fields_str}
) WITH (
    'connector' = 'kafka',
    'topic' = '{topic_name}',
    'properties.bootstrap.servers' = '{brokers}',
    'format' = 'json',
    'scan.startup.mode' = 'latest-offset'
);"""

    def _generate_sink_ddl(self, sink_table: str, schema: InferredSchema, config: HologresConfig) -> str:
        sink_table_name = f"hologres_sink_{sink_table}"

        fields = []
        for field in schema.fields:
            flink_type = self.TYPE_MAPPING.get(field.type, 'STRING')
            fields.append(f"    {field.name} {flink_type}")

        fields_str = ",\n".join(fields)

        return f"""CREATE TABLE {sink_table_name} (
{fields_str}
) WITH (
    'connector' = 'hologres',
    'dbname' = '{config.database}',
    'tablename' = '{sink_table}',
    'username' = '{config.user}',
    'password' = '{config.password}',
    'endpoint' = '{config.host}:{config.port}'
);"""

    def _generate_insert_sql(self, topic_name: str, sink_table: str, schema: InferredSchema) -> str:
        source_table = f"kafka_source_{topic_name}"
        sink_table_name = f"hologres_sink_{sink_table}"

        field_names = [field.name for field in schema.fields]
        fields_str = ", ".join(field_names)

        return f"""INSERT INTO {sink_table_name}
SELECT {fields_str}
FROM {source_table};"""
```

**代码行数**：~200 行

---

### 3.8 业务服务 (service.py)

**职责**：主流程编排

**核心代码**：
```python
import json
from typing import Optional
from .config import ConfigManager
from .database import HologresDAO
from .kafka_client import KafkaClient
from .type_inference import TypeInferencer
from .ddl_generator import DDLGenerator
from .sql_generator import FlinkSQLGenerator
from .models import FlinkSQLRecord
from .logger import get_logger

logger = get_logger(__name__)

class GeneratorService:
    def __init__(self, config_path: str = "config.yaml"):
        self.config_manager = ConfigManager(config_path)
        self.hologres_config = self.config_manager.get_hologres_config()
        self.dao = HologresDAO(self.hologres_config)

    def generate(self, topic_name: str, sink_table: Optional[str] = None) -> int:
        # 1. 获取 Topic 配置
        logger.info(f"查询 Topic 配置: {topic_name}")
        topic_config = self.dao.get_topic_config_by_name(topic_name)
        if not topic_config:
            raise ValueError(f"Topic 配置不存在: {topic_name}")

        # 2. 采样数据
        logger.info(f"连接 Kafka: {topic_config.kafka_brokers}")
        kafka_client = KafkaClient(topic_config.kafka_brokers, topic_name)
        logger.info(f"采样 Topic: {topic_name} (10 条)")
        messages = kafka_client.sample_messages(count=10)
        logger.info(f"采样完成，共 {len(messages)} 条数据")

        # 3. 推断类型
        logger.info("推断数据类型...")
        inferencer = TypeInferencer()
        schema = inferencer.infer_schema(messages)
        logger.info(f"推断完成，共 {len(schema.fields)} 个字段")

        # 4. 确定 sink 表名
        if not sink_table:
            sink_table = f"stg_kafka_{topic_name}_rt"

        # 5. 生成 DDL
        logger.info("生成 DDL...")
        ddl_gen = DDLGenerator()
        hologres_ddl = ddl_gen.generate_hologres_ddl(sink_table, schema)

        # 6. 生成 Flink SQL
        logger.info("生成 Flink SQL...")
        sql_gen = FlinkSQLGenerator()
        source_ddl, sink_ddl, insert_sql, full_sql = sql_gen.generate_full_sql(
            topic_name, sink_table, schema,
            topic_config.kafka_brokers, self.hologres_config
        )

        # 7. 检查表是否存在
        logger.info(f"检查 Sink 表: {sink_table}")
        if self.dao.table_exists(sink_table):
            logger.warning(f"表已存在: {sink_table}")
            raise ValueError(f"表已存在: {sink_table}，请使用不同的表名")

        # 8. 创建表
        logger.info("创建表...")
        self.dao.create_table(hologres_ddl)
        logger.info("创建表成功")

        # 9. 保存 SQL 记录
        logger.info("保存 SQL 记录...")
        record = FlinkSQLRecord(
            topic_id=topic_config.id,
            topic_name=topic_name,
            sink_table_name=sink_table,
            source_ddl=source_ddl,
            sink_ddl=sink_ddl,
            insert_sql=insert_sql,
            full_sql=full_sql,
            inferred_schema=json.loads(schema.model_dump_json()),
            sample_count=len(messages),
            status="generated"
        )
        record_id = self.dao.save_flink_sql_record(record)
        logger.info(f"保存成功，Record ID: {record_id}")

        return record_id

    def __del__(self):
        if hasattr(self, 'dao'):
            self.dao.close()
```

**代码行数**：~200 行

---

### 3.9 CLI 实现 (cli.py)

**职责**：命令行接口

**核心代码**：
```python
import click
from .service import GeneratorService
from .logger import get_logger

logger = get_logger(__name__)

@click.group()
def cli():
    """Kafka-Flink-Hologres 自动化工具"""
    pass

@cli.command()
@click.option('--topic-name', required=True, help='Kafka Topic 名称')
@click.option('--sink-table', default=None, help='Hologres Sink 表名')
@click.option('--config', default='config.yaml', help='配置文件路径')
def generate(topic_name: str, sink_table: str, config: str):
    """生成 Flink SQL"""
    try:
        service = GeneratorService(config)
        record_id = service.generate(topic_name, sink_table)
        click.echo(f"[SUCCESS] 生成成功！Record ID: {record_id}")
    except Exception as e:
        logger.error(f"生成失败: {e}")
        click.echo(f"[ERROR] {e}", err=True)
        raise click.Abort()

if __name__ == '__main__':
    cli()
```

**代码行数**：~80 行

---

### 3.10 日志配置 (logger.py)

**职责**：配置日志

**核心代码**：
```python
import logging
import os
from pathlib import Path

def get_logger(name: str) -> logging.Logger:
    logger = logging.getLogger(name)

    if not logger.handlers:
        logger.setLevel(logging.DEBUG)

        # 控制台处理器
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(levelname)s - %(message)s')
        console_handler.setFormatter(console_formatter)

        # 文件处理器
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        file_handler = logging.FileHandler(log_dir / 'app.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)

        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

    return logger
```

**代码行数**：~50 行

---

## 4. 依赖管理

### 4.1 pyproject.toml

```toml
[project]
name = "kafka-flink-tool"
version = "0.1.0"
description = "Kafka to Flink to Hologres automation tool"
requires-python = ">=3.11"
dependencies = [
    "pydantic>=2.0.0",
    "pyyaml>=6.0",
    "kafka-python>=2.0.0",
    "psycopg2-binary>=2.9.0",
    "click>=8.0.0",
]

[project.scripts]
kafka-flink-tool = "kafka_flink_tool.cli:cli"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

---

## 5. 数据库初始化脚本

### 5.1 scripts/init_database.sql

```sql
-- 创建 Kafka 配置表
CREATE TABLE IF NOT EXISTS kafka_topic_config (
    id BIGSERIAL PRIMARY KEY,
    topic_name TEXT NOT NULL UNIQUE,
    kafka_brokers TEXT NOT NULL,
    data_format TEXT NOT NULL DEFAULT 'json',
    description TEXT,
    is_active BOOLEAN NOT NULL DEFAULT true,
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_kafka_topic_config_topic_name ON kafka_topic_config(topic_name);
CREATE INDEX idx_kafka_topic_config_is_active ON kafka_topic_config(is_active);

-- 创建 Flink SQL 记录表
CREATE TABLE IF NOT EXISTS flink_sql_record (
    id BIGSERIAL PRIMARY KEY,
    topic_id BIGINT NOT NULL,
    topic_name TEXT NOT NULL,
    sink_table_name TEXT NOT NULL,
    source_ddl TEXT NOT NULL,
    sink_ddl TEXT NOT NULL,
    insert_sql TEXT NOT NULL,
    full_sql TEXT NOT NULL,
    inferred_schema JSONB,
    sample_count INTEGER NOT NULL DEFAULT 10,
    status TEXT NOT NULL DEFAULT 'generated',
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    deployed_at TIMESTAMPTZ,
    deprecated_at TIMESTAMPTZ
);

CREATE INDEX idx_flink_sql_record_topic_name ON flink_sql_record(topic_name);
CREATE INDEX idx_flink_sql_record_status ON flink_sql_record(status);
```

---

## 6. 开发计划

### 阶段 1：项目初始化（0.5 天）
- 创建项目目录结构
- 配置 pyproject.toml
- 初始化 uv 虚拟环境
- 创建配置文件和运行脚本

### 阶段 2：基础模块（0.5 天）
- 实现 `config.py`
- 实现 `models.py`
- 实现 `logger.py`

### 阶段 3：数据访问（1 天）
- 实现 `database.py`
- 实现 `kafka_client.py`
- 测试数据库和 Kafka 连接

### 阶段 4：核心逻辑（1 天）
- 实现 `type_inference.py`
- 实现 `ddl_generator.py`
- 实现 `sql_generator.py`

### 阶段 5：服务和 CLI（0.5 天）
- 实现 `service.py`
- 实现 `cli.py`
- 集成测试

### 阶段 6：文档和交付（0.5 天）
- 编写 README
- 最终测试
- 交付

**总计：4 天**

---

## 7. 代码规范

### 7.1 命名规范
- 模块名：小写+下划线（如 `type_inference.py`）
- 类名：大驼峰（如 `TypeInferencer`）
- 函数名：小写+下划线（如 `infer_schema`）
- 常量：全大写+下划线（如 `TYPE_MAPPING`）

### 7.2 类型注解
所有公开函数必须有类型注解

### 7.3 文档字符串
所有公开类和函数必须有文档字符串

### 7.4 代码行数限制
- 每个文件不超过 300 行
- 每个函数不超过 50 行

---

## 8. 测试策略

### 8.1 单元测试
- `test_type_inference.py`：测试类型推断逻辑
- `test_ddl_generator.py`：测试 DDL 生成
- `test_sql_generator.py`：测试 Flink SQL 生成

### 8.2 集成测试
- `test_service.py`：测试端到端流程

---

## 9. 对比：简化前后

| 项目 | 简化前 | 简化后 | 减少 |
|------|-------|-------|------|
| 总代码行数 | ~2000 行 | ~1300 行 | 35% |
| 模块文件数 | 14 个 | 10 个 | 29% |
| CLI 命令 | 4 个 | 1 个 | 75% |
| DAO 方法 | 8 个 | 4 个 | 50% |
| 配置项 | 15+ 项 | 5 项 | 67% |
| 开发时间 | 5.5 天 | 4 天 | 27% |

---

## 10. 核心优势

1. **极简设计**：只保留核心功能，删除所有非必要特性
2. **代码精简**：总代码量减少 35%，维护成本降低
3. **快速交付**：开发时间从 5.5 天减少到 4 天
4. **易于理解**：模块职责单一，接口清晰
5. **符合规范**：每个文件不超过 300 行，结构清晰

---

## 11. 风险控制

| 风险 | 应对措施 |
|------|---------|
| Kafka 连接失败 | 清晰的错误提示 |
| 采样数据不足 | 检查并抛出异常 |
| 表已存在 | 检查并提示用户 |
| 类型推断不准确 | 使用宽松类型策略 |
