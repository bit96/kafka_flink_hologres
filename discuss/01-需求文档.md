# Kafka-Flink-Hologres 自动化工具需求文档

## 1. 项目概述

### 1.1 项目背景
在实时数据处理场景中，从 Kafka 到 Hologres 的数据同步是常见需求。传统开发流程需要：
1. 手动分析 Kafka Topic 的数据结构
2. 在 Hologres 中创建目标表
3. 编写 Flink SQL 的 Source、Sink、INSERT 语句
4. 调试和验证

这个过程繁琐且容易出错，特别是在快速原型开发和测试阶段。

### 1.2 项目目标
开发一个自动化工具，能够：
- 自动读取 Kafka Topic 样本数据并分析其结构
- 自动推断字段类型
- 自动生成 Hologres 表 DDL
- 自动生成完整的 Flink SQL 代码
- 智能管理表的创建和 SQL 的存储

### 1.3 核心价值
- **提升效率**：将原本需要 30-60 分钟的手工操作缩短到 1-2 分钟
- **减少错误**：自动化类型推断和 SQL 生成，避免人为错误
- **标准化**：统一的命名规范和代码风格
- **可追溯**：所有生成的 SQL 都记录在数据库中，便于管理和审计

### 1.4 使用场景
- 快速原型开发：快速验证数据流转逻辑
- 测试环境搭建：批量创建测试用的数据同步任务
- 数据探索：快速了解 Kafka Topic 的数据结构

## 2. 功能需求

### 2.1 核心功能

#### 2.1.1 配置管理
- 从 Hologres 配置表读取 Kafka 连接信息
- 支持通过标识参数（如 topic_id 或 topic_name）定位配置
- 从配置文件读取 Hologres 连接信息

#### 2.1.2 数据采样
- 连接到指定的 Kafka Topic
- 读取最新的 10 条 JSON 格式消息
- 解析 JSON 数据结构

#### 2.1.3 类型推断
- 分析 10 条样本数据的字段类型
- 支持基础类型推断：
  - 整数 (INT, BIGINT)
  - 浮点数 (DOUBLE)
  - 字符串 (TEXT)
  - 布尔值 (BOOLEAN)
  - 时间戳 (TIMESTAMP)
- 处理类型冲突：使用宽松类型策略
  - 整数 + 浮点数 → DOUBLE
  - 数字 + 字符串 → TEXT
  - 任何类型 + NULL → 保持原类型且允许 NULL

#### 2.1.4 DDL 生成
- 生成 Hologres Sink 表的 CREATE TABLE 语句
- 字段默认允许 NULL
- 自动添加合理的注释
- 表名规范：`stg_kafka_{topic_name}_rt` 或用户指定

#### 2.1.5 Flink SQL 生成
生成完整的 Flink SQL 文件，包含：
1. **Source 表定义**
   - Kafka Connector 配置
   - JSON Format 配置
   - 字段定义（基于推断结果）

2. **Sink 表定义**
   - Hologres Connector 配置
   - 字段定义（与 Source 对应）

3. **INSERT 语句**
   - 从 Source 到 Sink 的数据转换逻辑
   - 字段映射

#### 2.1.6 表管理
- 检查 Hologres 中目标表是否存在
- 如果不存在，自动创建
- 如果存在，提供选项：
  - 提示已经有该表

#### 2.1.7 SQL 存储
- 将生成的 Flink SQL 保存到 Hologres 记录表
- 记录元数据：
  - 生成时间
  - Topic 信息
  - 表名
  - SQL 内容
  - 状态（已生成、已部署等）

### 2.2 非功能需求

#### 2.2.1 性能要求
- 数据采样时间 < 30 秒
- 整体处理时间 < 120 秒

#### 2.2.2 可靠性要求
- 暂不考虑复杂的异常处理（第一版）
- 基本的连接失败提示

#### 2.2.3 可维护性要求
- 代码模块化，单个文件不超过 300 行
- 使用强类型数据结构（Pydantic）
- 完善的日志记录

#### 2.2.4 可扩展性要求
- 预留扩展接口，便于后续：
  - 支持 Kafka 消息的 key 和元数据处理

## 3. 技术架构

### 3.1 技术栈
- **开发语言**：Python 3.11+
- **包管理**：uv
- **Kafka 客户端**：kafka-python
- **数据库驱动**：psycopg2（Hologres 兼容 PostgreSQL 协议）
- **数据验证**：Pydantic
- **日志**：Python logging 模块

### 3.2 架构模式
采用**独立 Python 工具**架构：
- 核心功能封装为独立的 Python 包
- 提供命令行接口（CLI）
- 后续可扩展为 MCP Server 或 Web API

### 3.3 系统架构图
```
┌─────────────────────────────────────────────────────────┐
│                    命令行接口 (CLI)                      │
└─────────────────────┬───────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────┐
│                   核心业务逻辑层                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │ 配置管理模块  │  │ 数据采样模块  │  │ 类型推断模块  │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │ DDL生成模块   │  │ SQL生成模块   │  │ 表管理模块    │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────┬───────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────┐
│                   数据访问层                             │
│  ┌──────────────┐              ┌──────────────┐         │
│  │ Kafka Client │              │ Hologres DAO │         │
│  └──────────────┘              └──────────────┘         │
└─────────────────────┬───────────────┬───────────────────┘
                      │               │
              ┌───────▼─────┐  ┌──────▼──────┐
              │    Kafka    │  │  Hologres   │
              └─────────────┘  └─────────────┘
```

### 3.4 工作流程
```
1. 读取配置
   ├─ 从配置文件读取 Hologres 连接信息
   └─ 从 Hologres 配置表读取 Kafka 连接信息

2. 数据采样
   ├─ 连接 Kafka
   ├─ 订阅指定 Topic
   ├─ 读取 10 条最新消息
   └─ 解析 JSON 数据

3. 类型推断
   ├─ 收集所有字段
   ├─ 分析每个字段的类型分布
   └─ 应用宽松类型策略

4. 生成 DDL
   ├─ 构建 CREATE TABLE 语句
   └─ 添加字段注释

5. 生成 Flink SQL
   ├─ 生成 Source 表定义
   ├─ 生成 Sink 表定义
   └─ 生成 INSERT 语句

6. 表管理
   ├─ 检查目标表是否存在
   └─ 根据策略创建或跳过

7. 保存 SQL
   ├─ 将 SQL 插入记录表
   └─ 返回生成结果
```

## 4. 输入输出规范

### 4.1 输入
**命令行参数**：
```bash
python -m kafka_flink_tool generate \
  --topic-id <topic_id> \
  --sink-table <table_name> \
  --create-mode <skip|create|overwrite>
```

**配置文件** (`config.yaml`):
```yaml
hologres:
  host: xxx.xxx.xxx.xxx
  port: 80
  database: my_database
  user: my_user
  password: my_password
```

### 4.2 输出
**标准输出**：
- 处理进度信息
- 生成的表名
- SQL 记录 ID

**Hologres 数据库**：
- 创建的 Sink 表
- 插入的 SQL 记录

**日志文件** (`logs/app.log`):
- 详细的执行日志
- 错误和警告信息

## 5. 约束和限制

### 5.1 第一版限制
- 仅支持 JSON 格式的 Kafka 消息
- 仅支持 Hologres 作为 Sink
- 基础类型推断，不支持嵌套结构
- 不处理 Kafka 消息的 key 和元数据
- 简化的异常处理

### 5.2 数据约束
- Kafka Topic 必须有至少 10 条消息
- JSON 消息必须是扁平结构（不支持嵌套对象和数组）
- 字段名必须符合 SQL 标识符规范

### 5.3 环境约束
- Python 3.11+
- 网络可访问 Kafka 和 Hologres
- Hologres 用户需要有建表权限

## 6. 后续扩展方向

### 6.1 功能扩展
- 支持嵌套 JSON 结构（自动展平或生成 JSON 类型字段）
- 支持 Kafka 消息的 key 和元数据
- 支持更多 Sink 类型（ClickHouse、MySQL 等）
- 支持更多数据格式（Avro、Protobuf 等）
- 支持自定义类型映射规则

### 6.2 架构扩展
- 封装为 MCP Server，集成到 Claude Desktop
- 提供 Web UI 界面
- 支持批量处理多个 Topic

### 6.3 智能化扩展
- 基于历史数据优化类型推断
- 自动建议分区键和索引
- 自动生成数据质量检查规则

## 7. 验收标准

### 7.1 功能验收
- [ ] 能够成功连接 Kafka 并读取样本数据
- [ ] 能够正确推断基础数据类型
- [ ] 能够生成语法正确的 Hologres DDL
- [ ] 能够生成可执行的 Flink SQL
- [ ] 能够在 Hologres 中创建表
- [ ] 能够将 SQL 保存到记录表

### 7.2 质量验收
- [ ] 单个 Python 文件不超过 300 行
- [ ] 所有数据结构使用 Pydantic 定义
- [ ] 代码通过 mypy 类型检查
- [ ] 关键函数有单元测试
- [ ] 有完整的日志记录

### 7.3 文档验收
- [ ] 需求文档完整
- [ ] 数据库设计文档完整
- [ ] 接口设计文档完整
- [ ] 实现方案文档完整
- [ ] README 包含使用说明

## 8. 项目里程碑

### 里程碑 1：文档完成（预计 1 天）
- 完成 4 个设计文档
- 评审并确认需求

### 里程碑 2：项目初始化（预计 0.5 天）
- 创建项目结构
- 配置开发环境
- 编写启动脚本

### 里程碑 3：核心功能开发（预计 2-3 天）
- 实现各个核心模块
- 编写单元测试

### 里程碑 4：集成测试（预计 1 天）
- 端到端测试
- 修复 bug

### 里程碑 5：文档和交付（预计 0.5 天）
- 完善 README
- 代码审查
- 交付

**总预计时间：5-6 天**
