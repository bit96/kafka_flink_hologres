# 接口设计文档

## 1. 概述

本文档定义了 Kafka-Flink-Hologres 自动化工具的核心接口设计，包括：
1. 命令行接口（CLI）
2. 配置文件格式
3. 核心模块接口
4. 数据结构定义

## 2. 命令行接口

### 2.1 主命令

```bash
python -m kafka_flink_tool [COMMAND] [OPTIONS]
```

### 2.2 子命令

#### 2.2.1 generate - 生成 Flink SQL

**功能**：根据 Kafka Topic 生成 Flink SQL 代码

**语法**：
```bash
python -m kafka_flink_tool generate \
  --topic-id <topic_id> \
  [--topic-name <topic_name>] \
  [--sink-table <table_name>] \
  [--create-mode <skip|create|overwrite>] \
  [--sample-count <count>] \
  [--config <config_file>]
```

**参数说明**：

| 参数 | 类型 | 必填 | 默认值 | 说明 |
|------|------|------|--------|------|
| --topic-id | INTEGER | 否* | - | Kafka Topic 配置 ID |
| --topic-name | STRING | 否* | - | Kafka Topic 名称 |
| --sink-table | STRING | 否 | {topic_name}_sink | Hologres Sink 表名 |
| --create-mode | ENUM | 否 | create | 表创建模式：skip/create/overwrite |
| --sample-count | INTEGER | 否 | 10 | 采样数据条数 |
| --config | STRING | 否 | config.yaml | 配置文件路径 |

*注：`--topic-id` 和 `--topic-name` 必须提供其中一个

**示例**：
```bash
# 使用 topic-id
python -m kafka_flink_tool generate --topic-id 1

# 使用 topic-name
python -m kafka_flink_tool generate --topic-name user_events

# 指定 sink 表名和创建模式
python -m kafka_flink_tool generate \
  --topic-name user_events \
  --sink-table user_data \
  --create-mode overwrite

# 指定采样数量
python -m kafka_flink_tool generate \
  --topic-id 1 \
  --sample-count 20
```

**输出**：
```
[INFO] 正在读取配置...
[INFO] 正在连接 Kafka: kafka-broker1:9092
[INFO] 正在采样 Topic: user_events (10 条)
[INFO] 采样完成，共获取 10 条数据
[INFO] 正在推断数据类型...
[INFO] 推断完成，共 5 个字段
[INFO] 正在生成 DDL...
[INFO] 正在生成 Flink SQL...
[INFO] 正在检查 Sink 表: user_events_sink
[INFO] 表不存在，正在创建...
[INFO] 表创建成功
[INFO] 正在保存 SQL 记录...
[SUCCESS] Flink SQL 生成成功！
  - Record ID: 123
  - Sink Table: user_events_sink
  - SQL 已保存到 Hologres
```

#### 2.2.2 list - 列出 SQL 记录

**功能**：列出已生成的 Flink SQL 记录

**语法**：
```bash
python -m kafka_flink_tool list \
  [--topic-name <topic_name>] \
  [--status <status>] \
  [--limit <limit>]
```

**参数说明**：

| 参数 | 类型 | 必填 | 默认值 | 说明 |
|------|------|------|--------|------|
| --topic-name | STRING | 否 | - | 过滤 Topic 名称 |
| --status | ENUM | 否 | - | 过滤状态：generated/deployed/failed/deprecated |
| --limit | INTEGER | 否 | 20 | 返回记录数量 |

**示例**：
```bash
# 列出所有记录
python -m kafka_flink_tool list

# 列出指定 Topic 的记录
python -m kafka_flink_tool list --topic-name user_events

# 列出已部署的记录
python -m kafka_flink_tool list --status deployed
```

#### 2.2.3 show - 查看 SQL 详情

**功能**：查看指定记录的 SQL 详情

**语法**：
```bash
python -m kafka_flink_tool show <record_id>
```

**示例**：
```bash
python -m kafka_flink_tool show 123
```

#### 2.2.4 init - 初始化数据库

**功能**：初始化 Hologres 数据库表结构

**语法**：
```bash
python -m kafka_flink_tool init [--config <config_file>]
```

**示例**：
```bash
python -m kafka_flink_tool init
```

## 3. 配置文件格式

### 3.1 配置文件路径
默认路径：`config.yaml`

### 3.2 配置文件结构

```yaml
# Hologres 数据库配置
hologres:
  host: "hologres-cn-hangzhou.aliyuncs.com"
  port: 80
  database: "my_database"
  user: "my_user"
  password: "my_password"

  # 连接池配置（可选）
  pool:
    min_size: 1
    max_size: 10
    timeout: 30

# 日志配置
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "logs/app.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # 日志轮转配置
  rotation:
    max_bytes: 10485760  # 10MB
    backup_count: 5

# Kafka 配置（可选，用于覆盖数据库中的配置）
kafka:
  # 全局超时设置
  timeout: 30

  # 消费者配置
  consumer:
    auto_offset_reset: "latest"  # earliest, latest
    enable_auto_commit: false
    session_timeout_ms: 30000

# 工具配置
tool:
  # 默认采样数量
  default_sample_count: 10

  # 默认表创建模式
  default_create_mode: "create"  # skip, create, overwrite

  # 类型推断配置
  type_inference:
    # 宽松类型策略
    use_loose_types: true

    # 字符串长度阈值（超过则使用 TEXT 而非 VARCHAR）
    string_length_threshold: 255
```

### 3.3 环境变量支持

配置文件支持环境变量替换：

```yaml
hologres:
  host: "${HOLOGRES_HOST}"
  port: "${HOLOGRES_PORT:80}"  # 默认值 80
  database: "${HOLOGRES_DATABASE}"
  user: "${HOLOGRES_USER}"
  password: "${HOLOGRES_PASSWORD}"
```

## 4. 核心模块接口

### 4.1 配置管理模块

#### 4.1.1 ConfigManager

```python
from typing import Optional
from pydantic import BaseModel

class HologresConfig(BaseModel):
    """Hologres 连接配置"""
    host: str
    port: int = 80
    database: str
    user: str
    password: str

class ConfigManager:
    """配置管理器"""

    def __init__(self, config_path: str = "config.yaml"):
        """初始化配置管理器"""
        pass

    def load_config(self) -> dict:
        """加载配置文件"""
        pass

    def get_hologres_config(self) -> HologresConfig:
        """获取 Hologres 配置"""
        pass
```

### 4.2 数据访问模块

#### 4.2.1 HologresDAO

```python
from typing import Optional, List
from pydantic import BaseModel

class KafkaTopicConfig(BaseModel):
    """Kafka Topic 配置"""
    id: int
    topic_name: str
    kafka_brokers: str
    kafka_group_id: Optional[str]
    kafka_security_protocol: Optional[str]
    kafka_sasl_mechanism: Optional[str]
    kafka_sasl_username: Optional[str]
    kafka_sasl_password: Optional[str]
    data_format: str
    description: Optional[str]
    is_active: bool

class FlinkSQLRecord(BaseModel):
    """Flink SQL 记录"""
    id: Optional[int] = None
    topic_id: int
    topic_name: str
    sink_table_name: str
    source_ddl: str
    sink_ddl: str
    insert_sql: str
    full_sql: str
    field_mapping: Optional[dict] = None
    inferred_schema: Optional[dict] = None
    sample_count: int = 10
    status: str = "generated"
    error_message: Optional[str] = None
    created_by: Optional[str] = None

class HologresDAO:
    """Hologres 数据访问对象"""

    def __init__(self, config: HologresConfig):
        """初始化 DAO"""
        pass

    def get_topic_config_by_id(self, topic_id: int) -> Optional[KafkaTopicConfig]:
        """根据 ID 获取 Topic 配置"""
        pass

    def get_topic_config_by_name(self, topic_name: str) -> Optional[KafkaTopicConfig]:
        """根据名称获取 Topic 配置"""
        pass

    def table_exists(self, table_name: str) -> bool:
        """检查表是否存在"""
        pass

    def create_table(self, ddl: str) -> None:
        """创建表"""
        pass

    def drop_table(self, table_name: str) -> None:
        """删除表"""
        pass

    def save_flink_sql_record(self, record: FlinkSQLRecord) -> int:
        """保存 Flink SQL 记录，返回记录 ID"""
        pass

    def list_flink_sql_records(
        self,
        topic_name: Optional[str] = None,
        status: Optional[str] = None,
        limit: int = 20
    ) -> List[FlinkSQLRecord]:
        """列出 Flink SQL 记录"""
        pass

    def get_flink_sql_record(self, record_id: int) -> Optional[FlinkSQLRecord]:
        """获取 Flink SQL 记录"""
        pass
```

#### 4.2.2 KafkaClient

```python
from typing import List, Dict, Any

class KafkaClient:
    """Kafka 客户端"""

    def __init__(self, config: KafkaTopicConfig):
        """初始化 Kafka 客户端"""
        pass

    def connect(self) -> None:
        """连接 Kafka"""
        pass

    def sample_messages(self, count: int = 10) -> List[Dict[str, Any]]:
        """采样消息"""
        pass

    def close(self) -> None:
        """关闭连接"""
        pass
```

### 4.3 类型推断模块

#### 4.3.1 TypeInferencer

```python
from typing import List, Dict, Any
from pydantic import BaseModel

class FieldSchema(BaseModel):
    """字段结构"""
    name: str
    type: str  # BIGINT, INTEGER, DOUBLE, TEXT, BOOLEAN, TIMESTAMP
    nullable: bool = True
    sample_values: List[Any] = []

class InferredSchema(BaseModel):
    """推断的数据结构"""
    fields: List[FieldSchema]
    sample_data_count: int
    inference_time: str

class TypeInferencer:
    """类型推断器"""

    def __init__(self, use_loose_types: bool = True):
        """初始化类型推断器"""
        pass

    def infer_schema(self, messages: List[Dict[str, Any]]) -> InferredSchema:
        """推断数据结构"""
        pass

    def infer_field_type(self, values: List[Any]) -> str:
        """推断字段类型"""
        pass
```

### 4.4 SQL 生成模块

#### 4.4.1 DDLGenerator

```python
class DDLGenerator:
    """DDL 生成器"""

    def generate_hologres_ddl(
        self,
        table_name: str,
        schema: InferredSchema
    ) -> str:
        """生成 Hologres 表 DDL"""
        pass
```

#### 4.4.2 FlinkSQLGenerator

```python
class FlinkSQLGenerator:
    """Flink SQL 生成器"""

    def generate_source_ddl(
        self,
        table_name: str,
        schema: InferredSchema,
        kafka_config: KafkaTopicConfig
    ) -> str:
        """生成 Flink Source 表 DDL"""
        pass

    def generate_sink_ddl(
        self,
        table_name: str,
        schema: InferredSchema,
        hologres_config: HologresConfig
    ) -> str:
        """生成 Flink Sink 表 DDL"""
        pass

    def generate_insert_sql(
        self,
        source_table: str,
        sink_table: str,
        schema: InferredSchema
    ) -> str:
        """生成 INSERT 语句"""
        pass

    def generate_full_sql(
        self,
        source_ddl: str,
        sink_ddl: str,
        insert_sql: str
    ) -> str:
        """生成完整的 Flink SQL"""
        pass
```

### 4.5 主流程模块

#### 4.5.1 FlinkSQLGenerator (主流程)

```python
class FlinkSQLGeneratorService:
    """Flink SQL 生成服务（主流程编排）"""

    def __init__(self, config_path: str = "config.yaml"):
        """初始化服务"""
        pass

    def generate(
        self,
        topic_id: Optional[int] = None,
        topic_name: Optional[str] = None,
        sink_table: Optional[str] = None,
        create_mode: str = "create",
        sample_count: int = 10
    ) -> int:
        """
        生成 Flink SQL

        Returns:
            生成的记录 ID
        """
        pass
```

## 5. 数据结构定义

### 5.1 类型映射表

#### 5.1.1 Python 类型 → Flink SQL 类型

| Python 类型 | Flink SQL 类型 | 说明 |
|------------|---------------|------|
| int (小) | INT | -2^31 ~ 2^31-1 |
| int (大) | BIGINT | -2^63 ~ 2^63-1 |
| float | DOUBLE | 双精度浮点数 |
| str | STRING | 字符串 |
| bool | BOOLEAN | 布尔值 |
| datetime | TIMESTAMP(3) | 毫秒精度时间戳 |

#### 5.1.2 Flink SQL 类型 → Hologres 类型

| Flink SQL 类型 | Hologres 类型 | 说明 |
|---------------|--------------|------|
| INT | INTEGER | 整数 |
| BIGINT | BIGINT | 长整数 |
| DOUBLE | DOUBLE PRECISION | 双精度浮点数 |
| STRING | TEXT | 文本 |
| BOOLEAN | BOOLEAN | 布尔值 |
| TIMESTAMP(3) | TIMESTAMPTZ | 带时区的时间戳 |

### 5.2 宽松类型策略

当同一字段出现多种类型时，按以下规则选择：

1. **整数 + 浮点数** → DOUBLE
2. **数字 + 字符串** → TEXT (STRING)
3. **布尔 + 其他** → TEXT (STRING)
4. **任何类型 + NULL** → 保持原类型，nullable = true

## 6. 错误处理

### 6.1 错误码定义

| 错误码 | 说明 | 处理建议 |
|-------|------|---------|
| E001 | 配置文件不存在 | 检查配置文件路径 |
| E002 | 配置文件格式错误 | 检查 YAML 语法 |
| E003 | Hologres 连接失败 | 检查网络和凭证 |
| E004 | Topic 配置不存在 | 检查 topic_id 或 topic_name |
| E005 | Kafka 连接失败 | 检查 Kafka 地址和认证 |
| E006 | 采样数据不足 | 确保 Topic 有足够消息 |
| E007 | JSON 解析失败 | 检查消息格式 |
| E008 | 表已存在 | 使用 overwrite 模式或更换表名 |
| E009 | 表创建失败 | 检查权限和 DDL 语法 |
| E010 | SQL 保存失败 | 检查数据库连接 |

### 6.2 异常类定义

```python
class KafkaFlinkToolError(Exception):
    """工具基础异常"""
    def __init__(self, code: str, message: str):
        self.code = code
        self.message = message
        super().__init__(f"[{code}] {message}")

class ConfigError(KafkaFlinkToolError):
    """配置错误"""
    pass

class DatabaseError(KafkaFlinkToolError):
    """数据库错误"""
    pass

class KafkaError(KafkaFlinkToolError):
    """Kafka 错误"""
    pass
```

## 7. 日志规范

### 7.1 日志级别

- **DEBUG**：详细的调试信息
- **INFO**：关键步骤信息
- **WARNING**：警告信息
- **ERROR**：错误信息

### 7.2 日志格式

```
2025-11-19 10:30:00,123 - kafka_flink_tool.service - INFO - 正在采样 Topic: user_events
```

### 7.3 关键日志点

1. 配置加载
2. 数据库连接
3. Kafka 连接
4. 数据采样开始/完成
5. 类型推断开始/完成
6. DDL 生成
7. 表创建
8. SQL 保存
9. 错误和异常
