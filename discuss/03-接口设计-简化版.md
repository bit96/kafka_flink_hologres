# 接口设计文档（简化版）

## 1. 概述

本文档定义了 Kafka-Flink-Hologres 自动化工具的核心接口设计，遵循"最小化实现"原则。

## 2. 命令行接口

### 2.1 generate 命令

**功能**：根据 Kafka Topic 生成 Flink SQL 代码

**语法**：
```bash
python -m kafka_flink_tool generate --topic-name <topic_name> [--sink-table <table_name>] [--config <config_file>]
```

**参数说明**：

| 参数 | 类型 | 必填 | 默认值 | 说明 |
|------|------|------|--------|------|
| --topic-name | STRING | 是 | - | Kafka Topic 名称 |
| --sink-table | STRING | 否 | stg_kafka_{topic_name}_rt | Hologres Sink 表名 |
| --config | STRING | 否 | config.yaml | 配置文件路径 |

**示例**：
```bash
# 基本用法
python -m kafka_flink_tool generate --topic-name user_events

```

**输出**：
```
[INFO] 读取配置完成
[INFO] 连接 Kafka: kafka-broker1:9092
[INFO] 采样 Topic: user_events (10 条)
[INFO] 推断完成，共 5 个字段
[INFO] 生成 DDL 完成
[INFO] 生成 Flink SQL 完成
[INFO] 检查 Sink 表: stg_kafka_user_events_rt
[INFO] 创建表成功
[INFO] 保存 SQL 记录完成
[SUCCESS] 生成成功！Record ID: 123
```

## 3. 配置文件

### 3.1 配置文件格式

**文件路径**：`config.yaml`

**内容**：
```yaml
hologres:
  host: "hologres-cn-hangzhou.aliyuncs.com"
  port: 80
  database: "my_database"
  user: "my_user"
  password: "my_password"
```

## 4. 核心模块接口

### 4.1 配置管理

```python
from pydantic import BaseModel

class HologresConfig(BaseModel):
    host: str
    port: int = 80
    database: str
    user: str
    password: str

class ConfigManager:
    def __init__(self, config_path: str = "config.yaml"):
        pass

    def get_hologres_config(self) -> HologresConfig:
        """获取 Hologres 配置"""
        pass
```

### 4.2 数据模型

```python
from typing import Optional, List, Any, Dict
from pydantic import BaseModel

class KafkaTopicConfig(BaseModel):
    """Kafka Topic 配置"""
    id: int
    topic_name: str
    kafka_brokers: str
    data_format: str
    description: Optional[str]
    is_active: bool

class FieldSchema(BaseModel):
    """字段结构"""
    name: str
    type: str  # BIGINT, INTEGER, DOUBLE, TEXT, BOOLEAN, TIMESTAMP
    nullable: bool = True

class InferredSchema(BaseModel):
    """推断的数据结构"""
    fields: List[FieldSchema]
    sample_data_count: int

class FlinkSQLRecord(BaseModel):
    """Flink SQL 记录"""
    id: Optional[int] = None
    topic_id: int
    topic_name: str
    sink_table_name: str
    source_ddl: str
    sink_ddl: str
    insert_sql: str
    full_sql: str
    inferred_schema: Optional[dict] = None
    sample_count: int = 10
    status: str = "generated"
```

### 4.3 数据访问

```python
class HologresDAO:
    def __init__(self, config: HologresConfig):
        pass

    def get_topic_config_by_name(self, topic_name: str) -> Optional[KafkaTopicConfig]:
        """根据名称获取 Topic 配置"""
        pass

    def table_exists(self, table_name: str) -> bool:
        """检查表是否存在"""
        pass

    def create_table(self, ddl: str) -> None:
        """创建表"""
        pass

    def save_flink_sql_record(self, record: FlinkSQLRecord) -> int:
        """保存 Flink SQL 记录，返回记录 ID"""
        pass
```

### 4.4 Kafka 客户端

```python
class KafkaClient:
    def __init__(self, brokers: str, topic_name: str):
        pass

    def sample_messages(self, count: int = 10) -> List[Dict[str, Any]]:
        """采样消息"""
        pass
```

### 4.5 类型推断

```python
class TypeInferencer:
    def infer_schema(self, messages: List[Dict[str, Any]]) -> InferredSchema:
        """推断数据结构"""
        pass
```

### 4.6 DDL 生成

```python
class DDLGenerator:
    def generate_hologres_ddl(self, table_name: str, schema: InferredSchema) -> str:
        """生成 Hologres 表 DDL"""
        pass
```

### 4.7 Flink SQL 生成

```python
class FlinkSQLGenerator:
    def generate_full_sql(
        self,
        topic_name: str,
        sink_table: str,
        schema: InferredSchema,
        kafka_brokers: str,
        hologres_config: HologresConfig
    ) -> tuple[str, str, str, str]:
        """
        生成完整的 Flink SQL

        Returns:
            (source_ddl, sink_ddl, insert_sql, full_sql)
        """
        pass
```

### 4.8 主流程服务

```python
class GeneratorService:
    def __init__(self, config_path: str = "config.yaml"):
        pass

    def generate(self, topic_name: str, sink_table: Optional[str] = None) -> int:
        """
        生成 Flink SQL

        Returns:
            生成的记录 ID
        """
        pass
```

## 5. 类型映射

### 5.1 Python → Flink SQL → Hologres

| Python 类型 | Flink SQL 类型 | Hologres 类型 |
|------------|---------------|--------------|
| int | BIGINT | BIGINT |
| float | DOUBLE | DOUBLE PRECISION |
| str | STRING | TEXT |
| bool | BOOLEAN | BOOLEAN |
| datetime | TIMESTAMP(3) | TIMESTAMPTZ |

### 5.2 宽松类型策略

- 整数 + 浮点数 → DOUBLE
- 数字 + 字符串 → TEXT
- 任何类型 + NULL → nullable = true

## 6. 工作流程

```
1. 加载配置 (config.yaml)
2. 查询 Topic 配置 (kafka_topic_config 表)
3. 连接 Kafka 并采样 10 条消息
4. 推断字段类型
5. 生成 Hologres DDL
6. 生成 Flink SQL (Source + Sink + INSERT)
7. 检查表是否存在
   - 存在：提示并退出
   - 不存在：创建表
8. 保存 SQL 记录到 flink_sql_record 表
9. 返回记录 ID
```

## 7. 错误处理

使用 Python 标准异常，关键错误点：
- 配置文件读取失败
- Hologres 连接失败
- Topic 配置不存在
- Kafka 连接失败
- 采样数据不足
- JSON 解析失败
- 表已存在
- 表创建失败

## 8. 日志

使用 Python logging 模块：
- 输出到控制台（INFO 级别）
- 输出到文件 `logs/app.log`（DEBUG 级别）
- 格式：`%(asctime)s - %(levelname)s - %(message)s`
